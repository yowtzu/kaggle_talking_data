{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yowtzu/anaconda/lib/python3.5/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n",
      "/Users/yowtzu/anaconda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import zipfile\n",
    "import time\n",
    "import shutil\n",
    "from sklearn.metrics import log_loss\n",
    "from pygeocoder import Geocoder, GeocoderError\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = pd.read_csv(\"data/events.csv\", dtype={'device_id': np.str})\n",
    "events = events.sort_values(['device_id','timestamp'])\n",
    "events['timestamp'] = pd.to_datetime(events.timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and save the latitude and longitude mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coord_to_loc_map = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in events.itertuples():\n",
    "    key = (row.latitude, row.longitude)\n",
    "    if key not in coord_to_loc_map.keys():\n",
    "        try:\n",
    "            print(key)\n",
    "            coord_to_loc_map[key] = Geocoder.reverse_geocode(row.latitude, row.longitude)\n",
    "            time.sleep(1)\n",
    "        except GeocoderError as e:\n",
    "            coord_to_loc_map[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coord_to_loc_data_map = {k:v.data for k,v in coord_to_loc_map.items() if v}\n",
    "coord_to_loc_address_map = {k:v.formatted_address for k,v in coord_to_loc_map.items() if v}\n",
    "\n",
    "with open('data/coord_to_loc_data_map.pickle', 'wb') as handle:\n",
    "    pickle.dump(coord_to_loc_data_map, handle)\n",
    "    \n",
    "with open('data/coord_to_loc_address_map.pickle', 'wb') as handle:\n",
    "    pickle.dump(coord_to_loc_address_map, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/coord_to_loc_address_map.pickle', 'rb') as handle:\n",
    "    coord_to_loc_address_map = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = list()\n",
    "for k,v in coord_to_loc_address_map.items():\n",
    "    if v:\n",
    "        country = None\n",
    "        route = None\n",
    "        qu = None\n",
    "        shi = None\n",
    "        sheng = None\n",
    "        location = list(v.upper().split(', '))\n",
    "        if(len(location) <= 3):\n",
    "            country = \"OTHER\"\n",
    "        else:\n",
    "            for item in location[::-1]:\n",
    "                if item=='CHINA':\n",
    "                    country = item\n",
    "                if item.endswith('DAO') or item.endswith('LU') or item.endswith('JIE') or item.endswith('ROAD'):\n",
    "                    route = item\n",
    "                elif item.endswith('QU') or item.endswith('XIAN'):\n",
    "                    qu = item\n",
    "                elif item.endswith('SHI'):\n",
    "                    shi = item\n",
    "                elif item.endswith('SHENG'):\n",
    "                    sheng = item\n",
    "        item = np.array([k[0], k[1], route, qu, shi, sheng, country])\n",
    "        res.append(item)\n",
    "address = pd.DataFrame(res, columns=['latitude', 'longitude', 'route', 'qu', 'shi', 'sheng', 'country'])\n",
    "address.latitude = address.latitude.astype('float')\n",
    "address.longitude = address.longitude.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2381.0141034018447"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from collections import namedtuple\n",
    "\n",
    "def haversine_distance(origin, destination):\n",
    "      \"\"\" Haversine formula to calculate the distance between two lat/long points on a sphere \"\"\"\n",
    "\n",
    "      radius = 6371 # FAA approved globe radius in km\n",
    "\n",
    "      dlat = math.radians(destination[0]-origin[0])\n",
    "      dlon = math.radians(destination[1]-origin[1])\n",
    "      a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(origin[0])) \\\n",
    "          * math.cos(math.radians(destination[0])) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "      c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "      d = radius * c\n",
    "\n",
    "      # Return distance in km\n",
    "      return d\n",
    "\n",
    "LatLng = namedtuple('LatLng', 'lat, lng')\n",
    "  \n",
    "origin = (51.3, -0.1275) # London\n",
    "destination = (37.966667, 23.716667) # Athens\n",
    "\n",
    "haversine_distance(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events = events.sort_values(['device_id','timestamp'])\n",
    "\n",
    "events.ix[events.longitude==0, 'longitude']=np.nan\n",
    "events.ix[events.latitude==0, 'latitude']=np.nan\n",
    "events['longitude'] = events.groupby(['device_id'])['longitude'].fillna(method='ffill').fillna(method='backfill')\n",
    "events['latitude'] = events.groupby(['device_id'])['latitude'].fillna(method='ffill').fillna(method='backfill')\n",
    "\n",
    "events[\"min_latitude\"] = events.groupby('device_id')[\"latitude\"].transform(min)\n",
    "events[\"max_latitude\"] = events.groupby('device_id')[\"latitude\"].transform(max)\n",
    "events[\"min_longitude\"] = events.groupby('device_id')[\"longitude\"].transform(min)\n",
    "events[\"max_longitude\"] = events.groupby('device_id')[\"longitude\"].transform(max)\n",
    "\n",
    "events['longitude'] = events.groupby(['device_id'])['longitude'].transform(np.median)\n",
    "# should use mode and find out the distance travels instead\n",
    "events['latitude'] = events.groupby(['device_id'])['latitude'].transform(np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_location = events[[\"min_latitude\", \"max_latitude\", \"min_longitude\", \"max_longitude\"]].drop_duplicates()\n",
    "unique_location[\"distance\"] = unique_location.apply(\n",
    "    lambda row: haversine_distance((row.min_latitude, row.min_longitude), (row.max_latitude, row.max_longitude)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events[\"hour\"] = pd.DatetimeIndex(events.timestamp).hour\n",
    "events[\"day_or_night\"] = 'day'\n",
    "events.ix[events.hour >=23 | (events.hour <= 7) , \"day_or_night\"] = 'night'\n",
    "events.drop('hour', axis=1, inplace=True)\n",
    "\n",
    "tmp = pd.DataFrame(events.groupby('device_id').apply(lambda x: np.log(1+len(x))).rename('counts'))\n",
    "events = events.join(tmp, on=['device_id'])\n",
    "\n",
    "tmp = pd.DataFrame(events.ix[events.day_or_night=='day',:].groupby('device_id').apply(lambda x: np.log(1+len(x))).rename('day_counts'))\n",
    "events = events.join(tmp, on=['device_id'])\n",
    "\n",
    "tmp = pd.DataFrame(events.ix[events.day_or_night=='night',:].groupby('device_id').apply(lambda x: np.log(1+len(x))).rename('night_counts'))\n",
    "events = events.join(tmp, on=['device_id'])\n",
    "events.drop(['day_or_night', 'timestamp'], axis=1, inplace=True)\n",
    "events = events.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events[\"day_counts\"] = events[\"day_counts\"].fillna(np.log(1))\n",
    "events[\"night_counts\"] = events[\"night_counts\"].fillna(np.log(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events = events.merge(unique_location, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = events.merge(address, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(2016)\n",
    "\n",
    "def run_xgb(train, test, features, target, random_state=0):\n",
    "    eta = 0.05\n",
    "    max_depth = 7\n",
    "    subsample = 0.7\n",
    "    colsample_bytree = 0.7\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": 12,\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"eta\": eta,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"subsample\": subsample,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": random_state,\n",
    "    }\n",
    "    num_boost_round = 500\n",
    "    early_stopping_rounds = 50\n",
    "    test_size = 0.3\n",
    "\n",
    "    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n",
    "    print('Length train:', len(X_train.index))\n",
    "    print('Length valid:', len(X_valid.index))\n",
    "    y_train = X_train[target]\n",
    "    y_valid = X_valid[target]\n",
    "    dtrain = xgb.DMatrix(X_train[features], y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    check = gbm.predict(xgb.DMatrix(X_valid[features]), ntree_limit=gbm.best_iteration)\n",
    "    score = log_loss(y_valid.tolist(), check)\n",
    "\n",
    "    print(\"Predict test set...\")\n",
    "    test_prediction = gbm.predict(xgb.DMatrix(test[features]), ntree_limit=gbm.best_iteration)\n",
    "\n",
    "    print('Training time: {} minutes'.format(round((time.time() - start_time)/60, 2)))\n",
    "    return test_prediction.tolist(), score\n",
    "\n",
    "\n",
    "def create_submission(score, test, prediction):\n",
    "    # Make Submission\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n",
    "    print('Writing submission: ', sub_file)\n",
    "    f = open(sub_file, 'w')\n",
    "    f.write('device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+\\n')\n",
    "    total = 0\n",
    "    test_val = test['device_id'].values\n",
    "    for i in range(len(test_val)):\n",
    "        str1 = str(test_val[i])\n",
    "        for j in range(12):\n",
    "            str1 += ',' + str(prediction[i][j])\n",
    "        str1 += '\\n'\n",
    "        total += 1\n",
    "        f.write(str1)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def map_column(table, f):\n",
    "    labels = sorted(table[f].unique())\n",
    "    mappings = dict()\n",
    "    for i in range(len(labels)):\n",
    "        mappings[labels[i]] = i\n",
    "    table = table.replace({f: mappings})\n",
    "    return table\n",
    "\n",
    "\n",
    "def read_train_test():\n",
    "    # Events\n",
    "    print('Read events...')\n",
    "    events = pd.read_csv(\"input/events.csv\", dtype={'device_id': np.str})\n",
    "    events['counts'] = events.groupby(['device_id'])['event_id'].transform('count')\n",
    "    events_small = events[['device_id', 'counts']].drop_duplicates('device_id', keep='first')\n",
    "\n",
    "    # Phone brand\n",
    "    print('Read brands...')\n",
    "    pbd = pd.read_csv(\"input/phone_brand_device_model.csv\", dtype={'device_id': np.str})\n",
    "    pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "    pbd = map_column(pbd, 'phone_brand')\n",
    "    pbd = map_column(pbd, 'device_model')\n",
    "\n",
    "    # Train\n",
    "    print('Read train...')\n",
    "    train = pd.read_csv(\"input/gender_age_train.csv\", dtype={'device_id': np.str})\n",
    "    train = map_column(train, 'group')\n",
    "    train = train.drop(['age'], axis=1)\n",
    "    train = train.drop(['gender'], axis=1)\n",
    "    train = pd.merge(train, pbd, how='left', on='device_id', left_index=True)\n",
    "    train = pd.merge(train, events_small, how='left', on='device_id', left_index=True)\n",
    "    train.fillna(-1, inplace=True)\n",
    "\n",
    "    # Test\n",
    "    print('Read test...')\n",
    "    test = pd.read_csv(\"input/gender_age_test.csv\", dtype={'device_id': np.str})\n",
    "    test = pd.merge(test, pbd, how='left', on='device_id', left_index=True)\n",
    "    test = pd.merge(test, events_small, how='left', on='device_id', left_index=True)\n",
    "    test.fillna(-1, inplace=True)\n",
    "\n",
    "    # Features\n",
    "    features = list(test.columns.values)\n",
    "    features.remove('device_id')\n",
    "\n",
    "    return train, test, features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train, test, features = read_train_test()\n",
    "#print('Length of train: ', len(train))\n",
    "#print('Length of test: ', len(test))\n",
    "#print('Features [{}]: {}'.format(len(features), sorted(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app = pd.read_csv(\"data/app_events.csv\", dtype={'app_id': np.str}).drop_duplicates()\n",
    "app_labels = pd.read_csv('data/app_labels.csv',  dtype={'app_id': np.str}).drop_duplicates()\n",
    "labels_categories = pd.read_csv('data/label_categories.csv').drop_duplicates()\n",
    "app_labels = pd.merge(app_labels, labels_categories, how='left', on='label_id')\n",
    "app = pd.merge(app, app_labels, how='left', on='app_id')\n",
    "app.drop(['is_installed', 'is_active', 'label_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events = pd.merge(events, app, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events['app_count'] = events.groupby('device_id')['app_id'].transform('count')\n",
    "events['app_category_count'] = events.groupby('device_id')['category'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events_small = events.drop(['event_id', 'app_id', 'category'], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events_small[\"country\"] = events_small.apply(lambda x: 0 if x[\"country\"]=='China' else 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events_small.ix[pd.isnull(events_small.shi),\"shi\"] = 'Unknown'\n",
    "events_small.ix[pd.isnull(events_small.sheng), 'sheng'] = 'Unknown'\n",
    "events_small.ix[pd.isnull(events_small.qu), 'qu'] = 'Unknown'\n",
    "events_small.ix[pd.isnull(events_small.route), 'route'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events_small.shi = events_small.shi.astype('str')\n",
    "events_small.sheng = events_small.sheng.astype('str')\n",
    "events_small.qu = events_small.qu.astype('str')\n",
    "events_small.route = events_small.route.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events_small = map_column(events_small, 'shi')\n",
    "events_small = map_column(events_small, 'sheng')\n",
    "events_small = map_column(events_small, 'qu')\n",
    "events_small = map_column(events_small, 'route')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events_small.drop(['min_latitude', 'max_latitude', 'min_longitude', 'max_longitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "events_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Phone brand\n",
    "print('Read brands...')\n",
    "pbd = pd.read_csv(\"data/phone_brand_device_model.csv\", dtype={'device_id': np.str})\n",
    "pbd.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "pbd = map_column(pbd, 'phone_brand')\n",
    "pbd = map_column(pbd, 'device_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "print('Read train...')\n",
    "train = pd.read_csv(\"data/gender_age_train.csv\", dtype={'device_id': np.str})\n",
    "train = map_column(train, 'group')\n",
    "train = train.drop(['age'], axis=1)\n",
    "train = train.drop(['gender'], axis=1)\n",
    "train = pd.merge(train, pbd, how='left', on='device_id', left_index=True)\n",
    "train = pd.merge(train, events_small, how='left', on='device_id', left_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "print('Read test...')\n",
    "test = pd.read_csv(\"data/gender_age_test.csv\", dtype={'device_id': np.str})\n",
    "test = pd.merge(test, pbd, how='left', on='device_id', left_index=True)\n",
    "test = pd.merge(test, events_small, how='left', on='device_id', left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features\n",
    "features = list(test.columns.values)\n",
    "features.remove('device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prediction, score = run_xgb(train, test, features, 'group')\n",
    "print(\"LS: {}\".format(round(score, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_submission(score, test, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
